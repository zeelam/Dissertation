{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train Your Own Key Points Detection Networks\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/69765965-fd65a700-1143-11ea-8804-cd1d33f2e824.png)\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to train your own KeyPoints detection network and do inference on pictures of traffic cone.\n",
    "\n",
    "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt install unzip\n",
    "print('Installing numpy...')\n",
    "! pip3 install numpy \n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip3 install tqdm \n",
    "print('Installing matplotlib...')\n",
    "! pip3 install matplotlib \n",
    "print('Installing dataset reader...')\n",
    "! pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' s clone our repo first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra.git\n",
    "\n",
    "! mv MIT-Driverless-CV-TrainingInfra/RektNet/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Training Dataset\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/RektNet_Dataset.zip\n",
    "! unzip -q RektNet_Dataset.zip\n",
    "! mv RektNet_Dataset dataset/ && rm RektNet_Dataset.zip\n",
    "print(\"Downloading Training and Validation Label\")\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/rektnet-training/mini_rektnet_label.csv && mv mini_rektnet_label.csv rektnet_label.csv && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from keypoint_net import KeypointNet\n",
    "from cross_ratio_loss import CrossRatioLoss\n",
    "from utils import Logger\n",
    "from utils import load_train_csv_dataset, prep_image, visualize_data, vis_tensor_and_save, calculate_distance, calculate_mean_distance\n",
    "from dataset import ConeDataset\n",
    "from keypoint_tutorial_util import print_tensor_stats, eval_model, print_kpt_L2_distance\n",
    "\n",
    "cv2.setRNGSeed(2)\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "visualization_tmp_path = \"/outputs/visualization/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully imported all packages and configured random seed to 17!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name=\"tutorial\"\n",
    "\n",
    "current_month = datetime.now().strftime('%B').lower()\n",
    "current_year = str(datetime.now().year)\n",
    "if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/')):\n",
    "    os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/'))\n",
    "output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/')\n",
    "\n",
    "save_file_name = 'logs/' + output_uri.split('/')[-2]\n",
    "sys.stdout = Logger(save_file_name + '.log')\n",
    "sys.stderr = Logger(save_file_name + '.error')\n",
    "\n",
    "# Training related config\n",
    "INPUT_SIZE = (80, 80) # dataset size\n",
    "KPT_KEYS = [\"top\", \"mid_L_top\", \"mid_R_top\", \"mid_L_bot\", \"mid_R_bot\", \"bot_L\", \"bot_R\"] # set up geometry loss keys\n",
    "intervals = int(2) # for normal training, set it to 4\n",
    "val_split = float(0.15) # training validation split ratio\n",
    "batch_size= int(8)\n",
    "num_epochs= int(4) # for normal training, set it to 1024\n",
    "train_csv = \"dataset/rektnet_label.csv\"\n",
    "dataset_path = \"dataset/RektNet_Dataset/\"\n",
    "vis_dataloader = False # visualize dataset\n",
    "save_checkpoints = True\n",
    "\n",
    "# Training related hyperparameter\n",
    "lr = 1e-1\n",
    "lr_gamma = 0.999\n",
    "geo_loss = True\n",
    "geo_loss_gamma_vert = 0\n",
    "geo_loss_gamma_horz = 0\n",
    "loss_type = \"l1_softargmax\" # loss function type: l2_softargmax|l2_heatmap|l1_softargmax\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "max_tolerance = 8\n",
    "tolerance = 0\n",
    "num_kpt=len(KPT_KEYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataloaders for train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, val_images, val_labels = load_train_csv_dataset(train_csv, validation_percent=val_split, keypoint_keys=KPT_KEYS, dataset_path=dataset_path, cache_location=\"./gs/\")\n",
    "\n",
    "train_dataset = ConeDataset(images=train_images, labels=train_labels, dataset_path=dataset_path, target_image_size=INPUT_SIZE, save_checkpoints=save_checkpoints, vis_dataloader=vis_dataloader)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle=False, num_workers=0)\n",
    "val_dataset = ConeDataset(images=val_images, labels=val_labels, dataset_path=dataset_path, target_image_size=INPUT_SIZE, save_checkpoints=save_checkpoints, vis_dataloader=vis_dataloader)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= 1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointNet(len(KPT_KEYS), INPUT_SIZE, onnx_mode=False)\n",
    "model = model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma)\n",
    "loss_func = CrossRatioLoss(loss_type, geo_loss, geo_loss_gamma_horz, geo_loss_gamma_vert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"EPOCH {epoch}\")\n",
    "    model.train()\n",
    "    total_loss = [0,0,0] # location/geometric/total\n",
    "    batch_num = 0\n",
    "\n",
    "    train_process = tqdm(train_dataloader)\n",
    "    for x_batch, y_hm_batch, y_points_batch, image_name, _ in train_process:\n",
    "        x_batch = x_batch.to(device, non_blocking=True)\n",
    "        y_hm_batch = y_hm_batch.to(device, non_blocking=True)\n",
    "        y_points_batch = y_points_batch.to(device, non_blocking=True)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Compute output and loss.\n",
    "        output = model(x_batch)\n",
    "        loc_loss, geo_loss, loss = loss_func(output[0], output[1], y_hm_batch, y_points_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loc_loss, geo_loss, loss = loc_loss.item(), geo_loss.item(), loss.item()\n",
    "        train_process.set_description(f\"Batch {batch_num}. Location Loss: {round(loc_loss,5)}. Geo Loss: {round(geo_loss,5)}. Total Loss: {round(loss,5)}\")\n",
    "        total_loss[0] += loc_loss\n",
    "        total_loss[1] += geo_loss\n",
    "        total_loss[2] += loss\n",
    "        batch_num += 1\n",
    "\n",
    "    print(f\"\\tTraining: MSE/Geometric/Total Loss: {round(total_loss[0]/batch_num,10)}/{round(total_loss[1]/batch_num,10)}/{round(total_loss[2]/batch_num,10)}\")\n",
    "    val_loc_loss, val_geo_loss, val_loss = eval_model(model=model, dataloader=val_dataloader, loss_function=loss_func, input_size=INPUT_SIZE)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        tolerance += 1\n",
    "\n",
    "    if save_checkpoints and epoch != 0 and (epoch + 1) % intervals == 0:\n",
    "        # Save the latest weights\n",
    "        gs_pt_uri = os.path.join(output_uri, \"{epoch}_loss_{loss}.pt\".format(epoch=epoch, loss=round(val_loss, 2)))\n",
    "        print(f'Saving model to {gs_pt_uri}')\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, gs_pt_uri)\n",
    "    if tolerance >= max_tolerance:\n",
    "        print(f\"Training is stopped due; loss no longer decreases. Epoch {best_epoch} is has the best validation loss.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download target image file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/test_kpt.png\n",
    "    \n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as pt\n",
    "\n",
    "image = cv2.imread(\"test_kpt.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "pt.fig = pt.figure(figsize=(5, 5))\n",
    "\n",
    "pt.imshow(image)\n",
    "pt.axis('off')\n",
    "pt.title('Keypoints Testing Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_kpt.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up config file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"pretrained_kpt.pt\"\n",
    "img = \"test_kpt.png\"\n",
    "img_size = int(80)\n",
    "output = \"outputs/visualization/\"\n",
    "flip = False\n",
    "rotate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = output\n",
    "model_filepath = model\n",
    "image_filepath = img\n",
    "\n",
    "img_name = '_'.join(image_filepath.split('/')[-1].split('.')[0].split('_')[-5:])\n",
    "\n",
    "image_size = (img_size, img_size)\n",
    "\n",
    "image = cv2.imread(image_filepath)\n",
    "\n",
    "image = prep_image(image=image,target_image_size=image_size)\n",
    "image = (image.transpose((2, 0, 1)) / 255.0)[np.newaxis, :]\n",
    "image = torch.from_numpy(image).type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KeypointNet()\n",
    "model.load_state_dict(torch.load(model_filepath).get('model'))\n",
    "model.eval()\n",
    "output = model(image)\n",
    "out = np.empty(shape=(0, output[0][0].shape[2]))\n",
    "for o in output[0][0]:\n",
    "    chan = np.array(o.cpu().data)\n",
    "    cmin = chan.min()\n",
    "    cmax = chan.max()\n",
    "    chan -= cmin\n",
    "    chan /= cmax - cmin\n",
    "    out = np.concatenate((out, chan), axis=0)\n",
    "cv2.imwrite(output_path + img_name + \"_hm.jpg\", out * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_filepath)\n",
    "h, w, _ = image.shape\n",
    "\n",
    "image = vis_tensor_and_save(image=image, h=h, w=w, tensor_output=output[1][0].cpu().data, image_name=img_name, output_uri=output_path)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "pt.fig = pt.figure(figsize=(5, 5))\n",
    "\n",
    "pt.imshow(image)\n",
    "pt.axis('off')\n",
    "pt.title('Keypoints Detection Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've finished all the content of this tutorial!\n",
    "Hope you enjoy playing with the our object detection model. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
    "\n",
    "## Reference\n",
    "[1] Kieran Strobel, Sibo Zhu, Raphael Chang and Skanda Koppula.\n",
    "**Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions**. In *IROS* 2020.\n",
    "[[paper]](https://arxiv.org/abs/2007.13971), [[code]](https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
