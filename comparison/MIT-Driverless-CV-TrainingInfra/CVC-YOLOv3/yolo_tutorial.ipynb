{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train Your Own Cone Detection Networks\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/22118253/70957091-fe06a480-2042-11ea-8c06-0fcc549fc19a.png\">\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to train your own YOLOv3-based traffic cone detection network and do inference on a video.\n",
    "\n",
    "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt install unzip\n",
    "print('Installing PyTorch...')\n",
    "! pip3 install torch \n",
    "print('Installing torchvision...')\n",
    "! pip3 install torchvision \n",
    "print('Installing numpy...')\n",
    "! pip3 install numpy \n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip3 install tqdm \n",
    "print('Installing matplotlib...')\n",
    "! pip3 install matplotlib \n",
    "print('Installing Tensorboard')\n",
    "! pip3 install tensorboardx\n",
    "print('Installing all the other required packages once for all')\n",
    "! sudo python3 setup.py install\n",
    "print('Installing video editor')\n",
    "! sudo apt install ffmpeg -y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' s clone our repo first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra.git\n",
    "\n",
    "! mv MIT-Driverless-CV-TrainingInfra/CVC-YOLOv3/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Training Dataset\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/YOLO_Dataset.zip\n",
    "! unzip -q YOLO_Dataset.zip\n",
    "! mv YOLO_Dataset dataset/ && rm YOLO_Dataset.zip\n",
    "print(\"Downloading YOLOv3 Sample Weights\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights \n",
    "print(\"Downloading Training and Validation Label\")\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train_mini_yolo.csv && mv train_mini_yolo.csv train.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate_mini_yolo.csv && mv validate_mini_yolo.csv validate.csv && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Pretrained YOLOv3 Weights File to Start Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import math\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import Darknet\n",
    "from utils.datasets import ImageLabelDataset\n",
    "from utils.utils import model_info, print_args, Logger, visualize_and_save_to_local,xywh2xyxy\n",
    "from yolo_tutorial_util import run_epoch\n",
    "import validate\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "##### section for all random seeds #####\n",
    "torch.manual_seed(2)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "########################################\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "num_cpu = multiprocessing.cpu_count() if cuda else 0\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.synchronize()\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully imported all packages and configured random seed to 17!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Related Config\n",
    "batch_size = int(5)\n",
    "optimizer_pick = \"Adam\"\n",
    "model_cfg = \"model_cfg/yolo_baseline.cfg\"\n",
    "weights_path = \"sample-yolov3.weights\"\n",
    "output_path = \"automatic\"\n",
    "dataset_path = \"dataset/YOLO_Dataset/\"\n",
    "num_epochs = int(2) # Set them to 2048 during full dataset training\n",
    "num_steps = 8388608\n",
    "checkpoint_interval = int(1) # How often you want to get evaluation metric during training\n",
    "val_tolerance = int(3)\n",
    "min_epochs = int(3)\n",
    "\n",
    "# Dataloader Related Config\n",
    "data_aug = False # toggle for image augmentation\n",
    "blur = False # Add blur to image\n",
    "salt = False # Add \"salt\" noise to image\n",
    "noise = False # Add noise to image\n",
    "contrast = False # Add High Contrast to image\n",
    "sharpen = False # Image Sharpen\n",
    "ts = True # Tiling and Scaling\n",
    "augment_affine = False # Affine\n",
    "augment_hsv = False # HSV\n",
    "lr_flip = False # left and right flip\n",
    "ud_flip = False # up and down flip\n",
    "\n",
    "# Training Hyperparameter Related Config\n",
    "momentum = float(0.9)\n",
    "gamma = float(0.95)\n",
    "lr = float(0.001)\n",
    "weight_decay = float(0.0)\n",
    "\n",
    "xy_loss = float(2)\n",
    "wh_loss= float(1.6)\n",
    "no_object_loss = float(25)\n",
    "object_loss = float(0.1)\n",
    "\n",
    "# Debugging/Visualization Related Config\n",
    "debug_mode = False\n",
    "upload_dataset = False\n",
    "vanilla_anchor = False\n",
    "vis_batch = int(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arguments = list(locals().items())\n",
    "\n",
    "print(\"Initializing model\")\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_path == \"automatic\":\n",
    "    current_month = datetime.now().strftime('%B').lower()\n",
    "    current_year = str(datetime.now().year)\n",
    "    if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])):\n",
    "        os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1]))\n",
    "    output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])\n",
    "else:\n",
    "    output_uri = output_path\n",
    "\n",
    "img_width, img_height = model.img_size()\n",
    "bw  = model.get_bw()\n",
    "validate_uri, train_uri = model.get_links()\n",
    "num_validate_images, num_train_images = model.num_images()\n",
    "conf_thresh, nms_thresh, iou_thresh = model.get_threshs()\n",
    "num_classes = model.get_num_classes()\n",
    "loss_constant = model.get_loss_constant()\n",
    "conv_activation = model.get_conv_activation()\n",
    "anchors = model.get_anchors()\n",
    "onnx_name = model.get_onnx_name()\n",
    "\n",
    "start_epoch = 0\n",
    "weights_path = weights_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "One of our main contributions to vanilla YOLOv3 is the custom data loader we implemented:\n",
    "\n",
    "Each set of training images from a specific sensor/lens/perspective combination is uniformly rescaled such that their landmark size distributions matched that of the camera system on the vehicle. Each training image was then padded if too small or split up into multiple images if too large.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/22118253/69765465-09e90000-1142-11ea-96b7-370868a0033b.png\" width=\"600\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tensorboard_data_dir:\n",
    "    print(\"Initializing data loaders\")\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(train_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=augment_hsv,\n",
    "                            augment_affine=augment_affine, num_images=num_train_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=lr_flip, ud_flip=ud_flip,vis_batch=vis_batch,data_aug=data_aug,blur=blur,salt=salt,noise=noise,contrast=contrast,sharpen=sharpen,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=(False if debug_mode else True),\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)\n",
    "    print(\"Num train images: \", len(train_data_loader.dataset))\n",
    "\n",
    "    validate_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(validate_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=False,\n",
    "                            augment_affine=False, num_images=num_validate_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=False, ud_flip=False,vis_batch=vis_batch,data_aug=False,blur=False,salt=False,noise=False,contrast=False,sharpen=False,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_pick == \"Adam\":\n",
    "    print(\"Using Adam Optimizer\")\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer_pick == \"SGD\":\n",
    "    print(\"Using SGD Optimizer\")\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "else:\n",
    "    raise Exception(f\"Invalid optimizer name: {optimizer_pick}\")\n",
    "print(\"Loading weights\")\n",
    "model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "\n",
    "# Set scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending Model to GPUs if we are in GPU mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Dance (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print('Using ', torch.cuda.device_count(), ' GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device, non_blocking=True)\n",
    "\n",
    "val_loss = 999  # using a high number for validation loss\n",
    "val_loss_counter = 0\n",
    "step = [0]  # wrapping in an array so it is mutable\n",
    "epoch = start_epoch\n",
    "while epoch < num_epochs and step[0] < num_steps:\n",
    "    epoch += 1\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    run_epoch(label_prefix=\"train\", data_loader=train_data_loader, epoch=epoch,\n",
    "                step=step, model=model, num_epochs=num_epochs, num_steps=num_steps,\n",
    "                optimizer=optimizer, device=device)\n",
    "    print('Completed epoch: ', epoch)\n",
    "    # Update best loss\n",
    "    if epoch % checkpoint_interval == 0 or epoch == num_epochs or step[0] >= num_steps:\n",
    "        # First, save the weights\n",
    "        save_weights_uri = os.path.join(output_uri, \"{epoch}.weights\".format(epoch=epoch))\n",
    "        model.save_weights(save_weights_uri)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating loss on validate data\")\n",
    "            epoch_losses, epoch_time_total, epoch_num_targets = run_epoch(\n",
    "                label_prefix=\"validate\", data_loader=validate_data_loader, epoch=epoch,\n",
    "                model=model, num_epochs=num_epochs, num_steps=num_steps, optimizer=None,\n",
    "                step=step, device=device)\n",
    "            avg_epoch_loss = epoch_losses[0] / epoch_num_targets\n",
    "            print('Average Validation Loss: {0:10.6f}'.format(avg_epoch_loss))\n",
    "\n",
    "            if avg_epoch_loss > val_loss and epoch > min_epochs:\n",
    "                val_loss_counter += 1\n",
    "                print(f\"Validation loss did not decrease for {val_loss_counter}\"\n",
    "                        f\" consecutive check(s)\")\n",
    "            else:\n",
    "                print(\"Validation loss decreased. Yay!!\")\n",
    "                val_loss_counter = 0\n",
    "                val_loss = avg_epoch_loss\n",
    "                ##### updating best result for optuna study #####\n",
    "                result = open(\"logs/result.txt\", \"w\" )\n",
    "                result.write(str(avg_epoch_loss))\n",
    "                result.close() \n",
    "                ###########################################\n",
    "            validate.validate(dataloader=validate_data_loader, model=model, device=device, step=step[0], bbox_all=False,debug_mode=debug_mode)\n",
    "            if val_loss_counter == val_tolerance:\n",
    "                print(\"Validation loss stopped decreasing over the last \" + str(val_tolerance) + \" checkpoints, creating onnx file\")\n",
    "                with tempfile.NamedTemporaryFile() as tmpfile:\n",
    "                    model.save_weights(tmpfile.name)\n",
    "                    weights_name = tmpfile.name\n",
    "                    cfg_name = os.path.join(tempfile.gettempdir(), model_cfg.split('/')[-1].split('.')[0] + '.tmp')\n",
    "                    onnx_gen = subprocess.call(['python3', 'yolo2onnx.py', '--cfg_name', cfg_name, '--weights_name', weights_name])\n",
    "                    save_weights_uri = os.path.join(output_uri, onnx_name)\n",
    "                    os.rename(weights_name, save_weights_uri)\n",
    "                    try:\n",
    "                        os.remove(onnx_name)\n",
    "                        os.remove(cfg_name)\n",
    "                    except:\n",
    "                        pass\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our full dataset accuracy metrics for detecting traffic cones on the racing track:\n",
    "\n",
    "| mAP | Recall | Precision |\n",
    "|----|----|----|\n",
    "| 89.35% | 92.77% | 86.94% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download target video file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/test_yolo_video.mp4\n",
    "\n",
    "! ffmpeg -i test_yolo_video.mp4 test.mp4 && rm test_yolo_video.mp4\n",
    "\n",
    "video_path = 'test.mp4'\n",
    "\n",
    "mp4 = open(video_path,'rb').read()\n",
    "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import cv2\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "from utils.nms import nms\n",
    "from utils.utils import calculate_padding\n",
    "from yolo_tutorial_util import single_img_detect, detect\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "detection_tmp_path = \"/tmp/detect/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up config file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"test.mp4\"\n",
    "output_path = \"outputs/visualization/\"\n",
    "weights_path = \"pretrained_yolo.weights\"\n",
    "conf_thres = float(0.8)\n",
    "nms_thres = float(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "\n",
    "# Load weights\n",
    "model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "detect(target_path, output_path, model, device=device, conf_thres=conf_thres, nms_thres=nms_thres, detection_tmp_path=detection_tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cd outputs/visualization/ && ffmpeg -i test.mp4 output.mp4 && rm test.mp4 && cd ../..\n",
    "\n",
    "video_path = \"outputs/visualization/output.mp4\"\n",
    "\n",
    "mp4 = open(video_path,'rb').read()\n",
    "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** Again, you can further improve the accuracy of the cone detection network by switching YOLOv3 backbone to the most recent published YOLOv4\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/22118253/70950893-e2de6980-202f-11ea-9a16-399579926ee5.gif\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "Congratulations! You've finished all the content of this tutorial!\n",
    "Hope you enjoy playing with the our object detection model. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
    "\n",
    "## Reference\n",
    "[1] Kieran Strobel, Sibo Zhu, Raphael Chang and Skanda Koppula.\n",
    "**Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions**. In *IROS* 2020.\n",
    "[[paper]](https://arxiv.org/abs/2007.13971), [[code]](https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
